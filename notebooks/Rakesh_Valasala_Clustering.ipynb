{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Rakesh Valasala Clustering Analysis Report\n",
    "\n",
    "This Jupyter Notebook demonstrates the application of several clustering algorithms on eCommerce transaction data to segment customers. The process involves preprocessing customer and transaction data, applying different clustering models, evaluating their performance, and generating visualizations and a detailed report.\n",
    "\n",
    "## Steps in the Notebook\n",
    "\n",
    "### 1. **Data Preprocessing**\n",
    "   - **Customer Data**: The `SignupDate` column is converted into a datetime format, and a new column `SignupYear` is extracted.\n",
    "   - **Transaction Data**: Aggregates data by `CustomerID`, summing the total value of transactions, counting transaction occurrences, and summing quantities purchased.\n",
    "   - **Merging Data**: The customer data is merged with the aggregated transaction data on the `CustomerID` column.\n",
    "   - **Feature Selection**: Key features are selected for clustering: `Region`, `SignupYear`, `TransactionCount`, `TotalValue`, and `TotalQuantity`. The categorical `Region` column is one-hot encoded.\n",
    "\n",
    "### 2. **Standardization**\n",
    "   - The feature set is standardized using `StandardScaler` to ensure that all features have a mean of 0 and a standard deviation of 1. This is important for distance-based clustering algorithms like KMeans.\n",
    "\n",
    "### 3. **Clustering Algorithms Applied**\n",
    "   The following clustering algorithms are applied to the standardized features:\n",
    "   - **KMeans**: A centroid-based clustering algorithm.\n",
    "   - **Agglomerative Clustering**: A hierarchical clustering method.\n",
    "   - **DBSCAN**: A density-based clustering algorithm.\n",
    "   - **Spectral Clustering**: A graph-based clustering method that uses eigenvalues.\n",
    "   - **MeanShift**: A non-parametric clustering algorithm.\n",
    "   - **Birch**: A clustering algorithm that builds a tree structure to partition the data.\n",
    "   - **OPTICS**: A density-based clustering algorithm that handles variable density clusters.\n",
    "   - **Gaussian Mixture Model**: A probabilistic model for clustering.\n",
    "\n",
    "### 4. **Evaluation of Clustering Performance**\n",
    "   - **Davies-Bouldin Index**: Used to evaluate the quality of the clusters. A lower value indicates better clustering.\n",
    "   - **Silhouette Score**: Measures how similar an object is to its own cluster compared to other clusters. A higher value indicates better-defined clusters.\n",
    "\n",
    "### 5. **Visualization**\n",
    "   For each clustering algorithm, the following visualizations are generated:\n",
    "   - **Cluster Visualization**: A scatter plot showing the clusters in a 2D space.\n",
    "   - **Pairplot**: A pairwise scatter plot showing relationships between features.\n",
    "   - **Silhouette Plot**: A horizontal bar plot to visualize the silhouette score for each data point.\n",
    "   - **Dendrogram**: A hierarchical clustering tree for the Agglomerative clustering algorithm.\n",
    "\n",
    "### 6. **Generating a PDF Report**\n",
    "   A detailed PDF report is generated, which includes:\n",
    "   - A summary of each clustering algorithm applied.\n",
    "   - The Davies-Bouldin Index for each algorithm.\n",
    "   - Visualizations of the clustering results.\n",
    "   - The report is saved as `Rakesh_Valasala_Clustering_Report.pdf`.\n",
    "\n",
    "### 7. **Saving Clustered Data**\n",
    "   The dataset with the cluster labels for each algorithm is saved as `Rakesh_Valasala_Clustering.csv`.\n",
    "\n",
    "---\n",
    "\n",
    "## Code Overview\n",
    "\n",
    "The following Python libraries are used in the notebook:\n",
    "- `pandas`: For data manipulation.\n",
    "- `sklearn`: For applying various clustering algorithms and evaluating clustering performance.\n",
    "- `matplotlib` & `seaborn`: For generating plots and visualizations.\n",
    "- `scipy`: For hierarchical clustering and dendrogram creation.\n",
    "- `fpdf`: For generating the PDF report.\n",
    "\n",
    "---\n",
    "\n",
    "## Expected Outputs\n",
    "\n",
    "- **Clustering Report PDF**: A comprehensive PDF report that includes Davies-Bouldin Indexes, Silhouette Scores, and visualizations.\n",
    "- **Clustered Data CSV**: A CSV file with the clustering labels for each algorithm applied to the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates the application of different clustering algorithms to segment eCommerce customers, providing insights into how these algorithms can be evaluated and visualized. The generated PDF report can serve as a useful tool for further analysis and decision-making.\n"
   ],
   "id": "34c7188f46091b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "4efe620be8955620"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T17:49:02.828562Z",
     "start_time": "2025-01-27T17:48:23.127138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering, MeanShift, Birch, OPTICS\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from fpdf import FPDF\n",
    "\n",
    "# Load Data\n",
    "customers_path = \"data/Customers.csv\"\n",
    "transactions_path = \"data/Transactions.csv\"\n",
    "\n",
    "customers = pd.read_csv(customers_path)\n",
    "transactions = pd.read_csv(transactions_path)\n",
    "\n",
    "# Preprocessing Customers Data\n",
    "customers['SignupDate'] = pd.to_datetime(customers['SignupDate'])\n",
    "customers['SignupYear'] = customers['SignupDate'].dt.year\n",
    "\n",
    "# Aggregating Transactions Data\n",
    "transaction_agg = transactions.groupby('CustomerID').agg({\n",
    "    'TotalValue': 'sum',\n",
    "    'TransactionID': 'count',\n",
    "    'Quantity': 'sum'\n",
    "}).rename(columns={\n",
    "    'TransactionID': 'TransactionCount',\n",
    "    'Quantity': 'TotalQuantity'\n",
    "}).reset_index()\n",
    "\n",
    "# Merge Data\n",
    "merged_data = pd.merge(customers, transaction_agg, on='CustomerID', how='left')\n",
    "merged_data.fillna(0, inplace=True)\n",
    "\n",
    "# Feature Selection\n",
    "features = merged_data[['Region', 'SignupYear', 'TransactionCount', 'TotalValue', 'TotalQuantity']]\n",
    "features = pd.get_dummies(features, columns=['Region'], drop_first=True)\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Clustering Algorithms Dictionary\n",
    "clustering_algorithms = {\n",
    "    'KMeans': KMeans(n_clusters=4, random_state=42),\n",
    "    'Agglomerative': AgglomerativeClustering(n_clusters=4),\n",
    "    'DBSCAN': DBSCAN(eps=0.5, min_samples=5),\n",
    "    'Spectral': SpectralClustering(n_clusters=4, random_state=42),\n",
    "    'MeanShift': MeanShift(),\n",
    "    'Birch': Birch(n_clusters=4),\n",
    "    'OPTICS': OPTICS(min_samples=5),\n",
    "    'GaussianMixture': GaussianMixture(n_components=4, random_state=42)\n",
    "}\n",
    "\n",
    "# Function to Apply Clustering Algorithm and Compute DB Index\n",
    "def apply_clustering_algorithm(algorithm, model, features):\n",
    "    model.fit(features)\n",
    "    labels = model.labels_ if hasattr(model, 'labels_') else model.predict(features)\n",
    "    if len(set(labels)) > 1:  # Ensure there is more than one cluster\n",
    "        db_index = davies_bouldin_score(features, labels)\n",
    "    else:\n",
    "        db_index = float('inf')  # Assign a high value if only one cluster\n",
    "    return labels, db_index\n",
    "\n",
    "# Prepare PDF\n",
    "pdf = FPDF()\n",
    "pdf.set_auto_page_break(auto=True, margin=15)\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "pdf.cell(200, 10, txt=\"Clustering Report: eCommerce Transactions Dataset\", ln=True, align=\"C\")\n",
    "pdf.ln(10)\n",
    "\n",
    "# Store visualizations and DB Indexes for each algorithm\n",
    "plots = []\n",
    "db_indexes = []\n",
    "\n",
    "# Apply each clustering algorithm and save results\n",
    "for algorithm, model in clustering_algorithms.items():\n",
    "    labels, db_index = apply_clustering_algorithm(algorithm, model, scaled_features)\n",
    "    merged_data[f'{algorithm}_Cluster'] = labels\n",
    "    db_indexes.append((algorithm, db_index))\n",
    "\n",
    "    # Cluster Visualization\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.scatterplot(\n",
    "        x=scaled_features[:, 0], y=scaled_features[:, 1],\n",
    "        hue=labels, palette='Set2'\n",
    "    )\n",
    "    plt.title(f\"{algorithm} Cluster Visualization\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plot_filename = f\"../output/{algorithm}_cluster_visualization.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    plots.append((f\"{algorithm} Cluster Visualization\", plot_filename))\n",
    "\n",
    "    # Pairplot for Pairwise Feature Relationships\n",
    "    pairplot_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
    "    pairplot_df['Cluster'] = labels\n",
    "    pairplot_filename = f\"../output/{algorithm}_pairplot.png\"\n",
    "    sns.pairplot(pairplot_df, hue='Cluster')\n",
    "    plt.savefig(pairplot_filename)\n",
    "    plt.close()\n",
    "    plots.append((f\"{algorithm} Pairplot\", pairplot_filename))\n",
    "\n",
    "    # Silhouette Plot\n",
    "    if len(set(labels)) > 1:  # Ensure there is more than one cluster\n",
    "        silhouette_avg = silhouette_score(scaled_features, labels)\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.title(f\"Silhouette Plot for {algorithm} - Score: {silhouette_avg:.3f}\")\n",
    "        plt.barh(range(len(labels)), silhouette_score(scaled_features, labels), color='skyblue')\n",
    "        silhouette_plot_filename = f\"../output/{algorithm}_silhouette_plot.png\"\n",
    "        plt.savefig(silhouette_plot_filename)\n",
    "        plt.close()\n",
    "        plots.append((f\"{algorithm} Silhouette Plot\", silhouette_plot_filename))\n",
    "\n",
    "    # Dendrogram for Agglomerative Clustering (if applicable)\n",
    "    if algorithm == 'Agglomerative':\n",
    "        Z = linkage(scaled_features, 'ward')\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        dendrogram(Z)\n",
    "        plt.title(f\"Dendrogram for {algorithm}\")\n",
    "        dendrogram_plot_filename = f\"../output/{algorithm}_dendrogram.png\"\n",
    "        plt.savefig(dendrogram_plot_filename)\n",
    "        plt.close()\n",
    "        plots.append((f\"{algorithm} Dendrogram\", dendrogram_plot_filename))\n",
    "\n",
    "# Add DB Indexes to PDF\n",
    "pdf.set_font(\"Arial\", size=10)\n",
    "for algorithm, db_index in db_indexes:\n",
    "    pdf.cell(200, 10, txt=f\"{algorithm} - Davies-Bouldin Index: {db_index:.4f}\", ln=True)\n",
    "\n",
    "# Add visualizations to PDF\n",
    "for title, plot in plots:\n",
    "    pdf.add_page()\n",
    "    pdf.cell(200, 10, txt=title, ln=True, align=\"C\")\n",
    "    pdf.image(plot, x=10, y=30, w=190)\n",
    "\n",
    "# Save PDF report\n",
    "pdf_output_path = \"output/Rakesh_Valasala_Clustering.pdf\"\n",
    "pdf.output(pdf_output_path)\n",
    "\n",
    "# Save merged data with cluster labels\n",
    "output_path = \"output/Rakesh_Valasala_Clustering.csv\"\n",
    "merged_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Clustering report saved to {pdf_output_path}\")\n",
    "print(f\"Clustered data saved to {output_path}\")"
   ],
   "id": "4725b9f739bf26b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering report saved to output/Rakesh_Valasala_Clustering.pdf\n",
      "Clustered data saved to output/Rakesh_Valasala_Clustering.csv\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
